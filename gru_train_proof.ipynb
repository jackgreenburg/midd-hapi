{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "773c5657-3170-40cb-9b55-7cf705d8f70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dartfs/rc/lab/V/VaickusL_slow/EDIT_Interns/anaconda_installation/envs/GreenburgPath/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pysnooper\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bff51c57-700d-4a96-9b6f-a96a507ab57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCATION = \"../new_pressure_injury_data\" \n",
    "\n",
    "lab_data_df = pd.read_csv(os.path.join(LOCATION, \"LAB_DATA.csv\"))\n",
    "\n",
    "path = os.path.join(LOCATION, \"new_indices_of_positive.json\")\n",
    "with open(path) as f:\n",
    "   pos_locs = [tuple(x) for x in json.load(f)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "451dd986-9679-40da-8083-0527ffc15ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_indices = lab_data_df.DE_ID_PAT_ID.isin((a for a,b in pos_locs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "930a08dd-6892-4e2a-93ed-3925af98c0c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80257, 129398)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lab_data_df[\"DE_ID_PAT_ID\"].drop_duplicates()), len(lab_data_df[[\"DE_ID_PAT_ID\", \"ENCOUNTER_COUNTER\"]].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20a4c89e-55f3-45ba-b76c-dd042d90885f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lab_data_df[\"VALUE_FLOAT\"] = pd.to_numeric(lab_data_df['VALUE'], errors='coerce')\n",
    "\n",
    "# drop encounters with more than two tests\n",
    "locs_with_more_than_two_tests = lab_data_df \\\n",
    "    .groupby(['DE_ID_PAT_ID', 'ENCOUNTER_COUNTER']) \\\n",
    "    .COMPONENT.count() \\\n",
    "    .to_frame().rename(columns={\"COMPONENT\": \"NUM_TESTS\"}) \\\n",
    "    .query(\"NUM_TESTS > 2\").index\n",
    "\n",
    "lab_data_df_processed = lab_data_df \\\n",
    "    .set_index(['DE_ID_PAT_ID', 'ENCOUNTER_COUNTER']) \\\n",
    "    .loc[locs_with_more_than_two_tests] \\\n",
    "    .reset_index() \\\n",
    "    \\\n",
    "    .set_index(['DE_ID_PAT_ID', 'ENCOUNTER_COUNTER','RESULT_DAY', 'COMPONENT'])[[\"VALUE_FLOAT\"]] \\\n",
    "    .dropna(axis='rows') \\\n",
    "    .query(\"COMPONENT in ['HGB', 'ALBUMIN', 'HEMOGLOBIN A1C', 'PREALBUMIN']\") \\\n",
    "    .sort_values('RESULT_DAY') \\\n",
    "    .groupby(['DE_ID_PAT_ID', 'ENCOUNTER_COUNTER','RESULT_DAY', 'COMPONENT']).VALUE_FLOAT.agg(\"mean\") \\\n",
    "    .unstack()\n",
    "# lab_data_df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97967b91-7407-4240-90ae-8385ea88b0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reindex_with_more_NaN(df, first_day, last_day):\n",
    "    \"\"\"\n",
    "    Reindex with days from (frist_day) to (last_day)\n",
    "    \"\"\"\n",
    "    indices = df.index.droplevel(\"RESULT_DAY\").drop_duplicates()\n",
    "    day_range = pd.RangeIndex(first_day, last_day + 1)\n",
    "    return df.reindex(\n",
    "        pd.DataFrame(None, index=indices, columns=day_range) \\\n",
    "            .reset_index() \\\n",
    "            .melt(id_vars=[\"DE_ID_PAT_ID\", \"ENCOUNTER_COUNTER\"], value_vars=day_range, var_name='RESULT_DAY') \\\n",
    "            .set_index([\"DE_ID_PAT_ID\", \"ENCOUNTER_COUNTER\", \"RESULT_DAY\"]) \\\n",
    "            .sort_index(level = 0).index\n",
    "     )\n",
    "\n",
    "first_day=1\n",
    "last_day=10\n",
    "\n",
    "X = reindex_with_more_NaN(lab_data_df_processed, first_day, last_day)  # lab_data_df_processed_reindexed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf36921-a46f-441e-88a7-a472af847916",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cce7fd2-2212-448c-a90e-70b17250dec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy_array(df, first_day, last_day):\n",
    "    return np.array_split(\n",
    "        df.values,\n",
    "        len(df)/(last_day-first_day+1)\n",
    "    )\n",
    "\n",
    "def split_and_impute(df, pos_locs, imputer, scaler=None):\n",
    "    X_train_IDs, X_test_IDs = train_test_split(df.index.get_level_values(0).drop_duplicates())\n",
    "\n",
    "    X_train_df = df.loc[X_train_IDs]\n",
    "    X_test_df = df.loc[X_test_IDs]\n",
    "\n",
    "    y_train = X_train_df.index.droplevel(2) \\\n",
    "        .drop_duplicates() \\\n",
    "        .isin(pos_locs) \\\n",
    "        .astype(np.int32)\n",
    "\n",
    "    y_test = X_test_df.index.droplevel(2) \\\n",
    "        .drop_duplicates() \\\n",
    "        .isin(pos_locs) \\\n",
    "        .astype(np.int32)\n",
    "\n",
    "    # impute\n",
    "    imputer.fit(X_train_df)\n",
    "\n",
    "    X_train_df = pd.DataFrame(imputer.transform(X_train_df)) \\\n",
    "        .set_index(X_train_df.index)\n",
    "    X_test_df = pd.DataFrame(imputer.transform(X_test_df)) \\\n",
    "        .set_index(X_test_df.index)\n",
    "\n",
    "    # scale\n",
    "    if scaler is not None:\n",
    "        X_train_df = pd.DataFrame(scaler.fit_transform(X_train_df))\n",
    "        X_train_df = pd.DataFrame(scaler.transform(X_test_df))\n",
    "\n",
    "    X_train = to_numpy_array(X_train_df, first_day, last_day)\n",
    "    X_test = to_numpy_array(X_test_df, first_day, last_day)\n",
    "    \n",
    "    \n",
    "    return (\n",
    "        *(torch.tensor(arr, dtype=torch.float32, requires_grad=True) for arr in [X_train, X_test]),\n",
    "        *(torch.tensor(arr, dtype=torch.float32, requires_grad=False) for arr in [y_train, y_test])\n",
    "    )\n",
    "\n",
    "# X_train, X_test, y_train, y_test = split_and_impute(\n",
    "#     X,\n",
    "#     pos_locs,\n",
    "#     SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "# )\n",
    "\n",
    "# (X_train.shape, sum(y_train)), (X_test.shape, sum(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf08ff4c-12ff-4ef7-a00d-9d882f016e66",
   "metadata": {},
   "source": [
    "### Creating fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55abaa56-8e39-4e9a-8547-ab030906e186",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALBUMIN  HEMOGLOBIN A1C  HGB    PREALBUMIN\n",
      "100.0    100.0           100.0  100.0         244\n",
      "0.0      0.0             0.0    0.0             2\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ipykernel_2903/3419404935.py:41: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525552411/work/torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  *(torch.tensor(arr, dtype=torch.float32, requires_grad=True) for arr in [X_train, X_test]),\n"
     ]
    }
   ],
   "source": [
    "# pd.MultiIndex.from_tuples(pos_locs) \\\n",
    "def fake_locs(pos_locs):\n",
    "    \"\"\"this just adds 1s to each row in the index\"\"\"\n",
    "    if isinstance(pos_locs, pd.MultiIndex):\n",
    "        AAA = pos_locs.to_frame()\n",
    "    else:\n",
    "        AAA = pd.DataFrame(pos_locs)\n",
    "    AAA[2] = [1]*len(pos_locs)\n",
    "    return pd.MultiIndex.from_frame(AAA)\n",
    "\n",
    "new_locs = X.index.droplevel(\"RESULT_DAY\").drop_duplicates().intersection(pos_locs)\n",
    "new_X = X*0\n",
    "\n",
    "# reduced size of dataset...\n",
    "trimmed_locs = new_X.index.droplevel(\"RESULT_DAY\").drop_duplicates()[:1000].union(new_locs)\n",
    "hope = pd.DataFrame([None]*len(trimmed_locs), index=fake_locs(trimmed_locs)) \\\n",
    "    .reset_index() \\\n",
    "    .rename(columns={2: \"RESULT_DAY\"}) \\\n",
    "    .set_index([\"DE_ID_PAT_ID\", \"ENCOUNTER_COUNTER\", \"RESULT_DAY\"])\n",
    "new_X = new_X.loc[reindex_with_more_NaN(hope, 1, 10).index]\n",
    "\n",
    "\n",
    "new_X.at[fake_locs(new_locs)] = 100\n",
    "print(new_X.value_counts())\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_and_impute(\n",
    "    new_X,\n",
    "    new_locs,\n",
    "    SimpleImputer(missing_values=np.nan, strategy='mean'),\n",
    "    preprocessing.MinMaxScaler()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9adf4c76-ec6e-4601-bf6a-48d4746724fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(list(zip(X_train, y_train.unsqueeze(1))), batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(list(zip(X_test, y_test.unsqueeze(1))), batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53c2d7f2-4013-4640-bb9d-37370fa39498",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # ?'cuda:2'\n",
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b2303567-c4bd-4865-959f-2235a3171eb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class GRUModel(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob=0):\n",
    "#         super(GRUModel, self).__init__()\n",
    "\n",
    "#         # Defining the number of layers and the nodes in each layer\n",
    "#         self.layer_dim = layer_dim\n",
    "#         self.hidden_dim = hidden_dim\n",
    "\n",
    "#         # GRU layers\n",
    "#         self.gru = nn.GRU(\n",
    "#             input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
    "#         )\n",
    "\n",
    "#         # Fully connected layer\n",
    "#         self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Initializing hidden state for first input with zeros\n",
    "#         h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
    "#         # Forward propagation by passing in the input and hidden state into the model\n",
    "#         out, _ = self.gru(x, h0.detach())\n",
    "#         # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "#         # so that it can fit into the fully connected layer\n",
    "#         out = out[:, -1, :]\n",
    "#         # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "#         out = self.fc(out)\n",
    "#         return out\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob=0):\n",
    "        super(LinearModel, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.layer_dim = layer_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        \n",
    "\n",
    "        self.l0 = nn.Linear(4, 1)\n",
    "        self.l1 = nn.Linear(10, 1)\n",
    "\n",
    "        # Fully connected layer\n",
    "        # self.fc = nn.Linear(32, output_dim)\n",
    "        \n",
    "        #  Sigmoid ?\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        # h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
    "        # Forward propagation by passing in the input and hidden state into the model\n",
    "        # out, _ = self.gru(x, h0.detach())\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        # print(out.size())\n",
    "        # out = out[:, -1, :]\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        # print(out.size())\n",
    "        \n",
    "        ##\n",
    "        # print(\"x.size() ->\", x.size())\n",
    "        # take last from\n",
    "        # out = x[:, -1, :]\n",
    "        \n",
    "        # print(\"selec\", out.size())\n",
    "        \n",
    "        out = self.l0(x).squeeze()\n",
    "        # print(\"after l0 ->\", out.size())\n",
    "        \n",
    "        out = self.l1(out)\n",
    "        # print(\"after l1 ->\", out.size())\n",
    "\n",
    "        # out = self.fc(out)\n",
    "        # print(\"after fc ->\", out.size())\n",
    "        \n",
    "        out = self.sig(out)\n",
    "        # print(\"after sig ->\",out.size())\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "class Optimization:\n",
    "    def __init__(self, model, loss_fn, optimizer):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "    \n",
    "    # @pysnooper.snoop()\n",
    "    # @pysnooper.snoop(watch=('self.model.state_dict()'))\n",
    "    def train_step(self, x, y):\n",
    "        # Sets model to train mode\n",
    "        self.model.train()\n",
    "\n",
    "        # Makes predictions\n",
    "        yhat = self.model(x)\n",
    "\n",
    "        # Computes loss\n",
    "        # print(\"before\", self.model.state_dict())\n",
    "        loss = self.loss_fn(y, yhat).squeeze()\n",
    "        # Computes gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updates parameters and zeroes gradients\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Returns the loss\n",
    "        return loss.item()\n",
    "    \n",
    "    # @pysnooper.snoop()\n",
    "    def train(self, train_loader, val_loader, batch_size=32, n_epochs=1, n_features=1):\n",
    "        # model_path = f'models/{self.model}_{datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}'\n",
    "\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            batch_losses = []\n",
    "            # print(\"batches(x100)->\", end=\"\")\n",
    "            for batches, (x_batch, y_batch) in enumerate(train_loader):\n",
    "                \n",
    "                # x_batch = x_batch.view([batch_size, -1, n_features]).to(device)\n",
    "                x_batch = x_batch.to(device)\n",
    "\n",
    "                # print(\"??\", x_batch.shape)\n",
    "                \n",
    "                y_batch = y_batch.to(device)\n",
    "                loss = self.train_step(x_batch, y_batch)\n",
    "                \n",
    "                batch_losses.append(loss)\n",
    "\n",
    "                if batches>0 and batches % 200 == 0:\n",
    "                    print(f\">[batch[{batches}]] Batch loss: {loss}\")\n",
    "                \n",
    "            training_loss = np.mean(batch_losses)\n",
    "            self.train_losses.append(training_loss)\n",
    "            # print(\"<-done\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                batch_val_losses = []\n",
    "                for x_val, y_val in val_loader:\n",
    "                    # x_val = x_val.view([batch_size, -1, n_features]).to(device)\n",
    "                    x_val = x_val.to(device)\n",
    "                    y_val = y_val.to(device)\n",
    "                    self.model.eval()\n",
    "                    yhat = self.model(x_val)\n",
    "                    val_loss = self.loss_fn(y_val, yhat).item()\n",
    "                    batch_val_losses.append(val_loss)\n",
    "                validation_loss = np.mean(batch_val_losses)\n",
    "                self.val_losses.append(validation_loss)\n",
    "\n",
    "            if (epoch <= 10) | (epoch % 50 == 0):\n",
    "                print(\n",
    "                    f\"[{epoch}/{n_epochs}] Training loss: {training_loss:.4f}\\t Validation loss: {validation_loss:.4f}\"\n",
    "                )\n",
    "\n",
    "        # torch.save(self.model.state_dict(), model_path)\n",
    "\n",
    "    def evaluate(self, test_loader, batch_size=1, n_features=1):\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            values = []\n",
    "            for x_test, y_test in test_loader:\n",
    "                # x_test = x_test.view([batch_size, -1, n_features]).to(device)\n",
    "    \n",
    "                x_test = x_test.to(device)\n",
    "                y_test = y_test.to(device)\n",
    "                self.model.eval()\n",
    "                yhat = self.model(x_test)\n",
    "                predictions.append(yhat.to(device).detach().cpu().numpy())\n",
    "                values.append(y_test.to(device).detach().cpu().numpy())\n",
    "\n",
    "        return predictions, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf2e7615-8eaf-44ae-bcd0-e595d1b1abdf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): LinearModel(\n",
      "    (l0): Linear(in_features=4, out_features=1, bias=True)\n",
      "    (l1): Linear(in_features=10, out_features=1, bias=True)\n",
      "    (sig): Sigmoid()\n",
      "  )\n",
      ")\n",
      "OrderedDict([('l0.weight', tensor([[-0.1348,  0.2135, -0.1253,  0.0395]], device='cuda:0')), ('l0.bias', tensor([0.3954], device='cuda:0')), ('l1.weight', tensor([[ 0.2106, -0.1290,  0.2045, -0.2302,  0.2786,  0.2670,  0.2277, -0.0564,\n",
      "          0.1304, -0.1761]], device='cuda:0')), ('l1.bias', tensor([0.2071], device='cuda:0'))])\n",
      "[1/300] Training loss: 58.4632\t Validation loss: 78.0981\n",
      "[2/300] Training loss: 58.1466\t Validation loss: 79.2114\n",
      "[3/300] Training loss: 58.4648\t Validation loss: 76.9850\n",
      "[4/300] Training loss: 58.7997\t Validation loss: 80.3258\n",
      "[5/300] Training loss: 58.1537\t Validation loss: 79.2113\n",
      "[6/300] Training loss: 57.8448\t Validation loss: 78.0997\n",
      "[7/300] Training loss: 58.4689\t Validation loss: 76.9834\n",
      "[8/300] Training loss: 58.4743\t Validation loss: 79.2117\n",
      "[9/300] Training loss: 58.1653\t Validation loss: 80.3255\n",
      "[10/300] Training loss: 57.8302\t Validation loss: 78.0980\n",
      "[50/300] Training loss: 58.4821\t Validation loss: 79.2133\n",
      "[100/300] Training loss: 57.8481\t Validation loss: 78.0982\n",
      "[150/300] Training loss: 58.7897\t Validation loss: 76.9847\n",
      "[200/300] Training loss: 58.7972\t Validation loss: 79.2131\n",
      "[250/300] Training loss: 58.4577\t Validation loss: 79.2133\n",
      "[300/300] Training loss: 58.4906\t Validation loss: 78.0992\n"
     ]
    }
   ],
   "source": [
    "bidirectional = 1\n",
    "input_size = X_train.shape[2]\n",
    "H_in = 16  # size of hidden state\n",
    "H_out = 1\n",
    "num_layers = 1\n",
    "\n",
    "model = nn.Sequential(\n",
    "          LinearModel(input_size, H_in, num_layers, H_out),\n",
    "        ).to(device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "#BCEWithLogitsLoss no\n",
    "# weight= torch.tensor()[.5], dtype=torch.float).to(device)\n",
    "opt = Optimization(\n",
    "    LinearModel(input_size, H_in, num_layers, H_out).to(device),\n",
    "    nn.BCELoss(),#weight),\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    ")\n",
    "print(opt.model.state_dict())\n",
    "opt.train(train_dataloader, test_dataloader, n_epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff4dd81-8763-4c04-9a87-879f0dcd53f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "p, v = opt.evaluate(test_dataloader)\n",
    "\n",
    "p_c = np.concatenate(p).squeeze()\n",
    "v_c = np.concatenate(v).squeeze()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GreenburgPath",
   "language": "python",
   "name": "greenburgpath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
